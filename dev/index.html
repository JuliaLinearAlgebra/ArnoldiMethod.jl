<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Home · ArnoldiMethod.jl</title><meta name="title" content="Home · ArnoldiMethod.jl"/><meta property="og:title" content="Home · ArnoldiMethod.jl"/><meta property="twitter:title" content="Home · ArnoldiMethod.jl"/><meta name="description" content="Documentation for ArnoldiMethod.jl."/><meta property="og:description" content="Documentation for ArnoldiMethod.jl."/><meta property="twitter:description" content="Documentation for ArnoldiMethod.jl."/><script data-outdated-warner src="assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="assets/documenter.js"></script><script src="search_index.js"></script><script src="siteinfo.js"></script><script src="../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href>ArnoldiMethod.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li class="is-active"><a class="tocitem" href>Home</a><ul class="internal"><li><a class="tocitem" href="#Installing"><span>Installing</span></a></li><li><a class="tocitem" href="#Example"><span>Example</span></a></li><li><a class="tocitem" href="#Partial-Schur-decomposition"><span>Partial Schur decomposition</span></a></li><li><a class="tocitem" href="#Partial-eigendecomposition"><span>Partial eigendecomposition</span></a></li><li><a class="tocitem" href="#Stopping-criterion"><span>Stopping criterion</span></a></li><li><a class="tocitem" href="#The-PartialSchur-and-History-structs"><span>The PartialSchur and History structs</span></a></li><li><a class="tocitem" href="#What-algorithm-is-ArnoldiMethod.jl?"><span>What algorithm is ArnoldiMethod.jl?</span></a></li><li><a class="tocitem" href="#Bringing-problems-to-standard-form"><span>Bringing problems to standard form</span></a></li><li><a class="tocitem" href="#Goal-of-this-package:-an-efficient,-pure-Julia-implementation"><span>Goal of this package: an efficient, pure Julia implementation</span></a></li><li><a class="tocitem" href="#Performance"><span>Performance</span></a></li><li><a class="tocitem" href="#Status"><span>Status</span></a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Home</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Home</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/JuliaLinearAlgebra/ArnoldiMethod.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/JuliaLinearAlgebra/ArnoldiMethod.jl/blob/master/docs/src/index.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="ArnoldiMethod.jl"><a class="docs-heading-anchor" href="#ArnoldiMethod.jl">ArnoldiMethod.jl</a><a id="ArnoldiMethod.jl-1"></a><a class="docs-heading-anchor-permalink" href="#ArnoldiMethod.jl" title="Permalink"></a></h1><p>ArnoldiMethod.jl provides an iterative method to find a few approximate  solutions to the eigenvalue problem in <em>standard form</em>:</p><p class="math-container">\[Ax = x\lambda,\]</p><p>where <span>$A$</span> is a general matrix of size <span>$n \times n$</span>; and <span>$x \in \mathbb{C}^n$</span> and <span>$\lambda \in \mathbb{C}$</span> are eigenvectors and eigenvalues respectively. By  <em>general matrix</em> we mean that <span>$A$</span> has no special structure. It can be symmetric or non-symmetric and either real or complex.</p><p>The method is <em>matrix-free</em>, meaning that it only requires multiplication with  the matrix <span>$A$</span>.</p><h2 id="Installing"><a class="docs-heading-anchor" href="#Installing">Installing</a><a id="Installing-1"></a><a class="docs-heading-anchor-permalink" href="#Installing" title="Permalink"></a></h2><p>In Julia open the package manager in the REPL via <code>]</code> and run:</p><pre><code class="language-julia hljs">(v1.6) pkg&gt; add ArnoldiMethod</code></pre><p>Then use the package.</p><pre><code class="language-julia hljs">using ArnoldiMethod</code></pre><p>The package exports just two functions:</p><ul><li><a href="#ArnoldiMethod.partialschur"><code>partialschur</code></a> to compute a stable basis for an eigenspace;</li><li><a href="#ArnoldiMethod.partialeigen"><code>partialeigen</code></a> to compute an eigendecomposition from a partial Schur decomposition.</li></ul><h2 id="Example"><a class="docs-heading-anchor" href="#Example">Example</a><a id="Example-1"></a><a class="docs-heading-anchor-permalink" href="#Example" title="Permalink"></a></h2><p>Here we compute the first ten eigenvalues and eigenvectors of a tridiagonal sparse matrix.</p><pre><code class="language-julia hljs">julia&gt; using ArnoldiMethod, LinearAlgebra, SparseArrays

julia&gt; A = spdiagm(
           -1 =&gt; fill(-1.0, 99),
            0 =&gt; fill(2.0, 100), 
            1 =&gt; fill(-1.0, 99)
       );

julia&gt; decomp, history = partialschur(A, nev=10, tol=1e-6, which=:SR);

julia&gt; decomp
PartialSchur decomposition (Float64) of dimension 10
eigenvalues:
10-element Array{Complex{Float64},1}:
 0.0009674354160236865 + 0.0im
  0.003868805732811139 + 0.0im
  0.008701304061962657 + 0.0im
   0.01546025527344699 + 0.0im
  0.024139120518486677 + 0.0im
    0.0347295035554728 + 0.0im
   0.04722115887278571 + 0.0im
   0.06160200160067088 + 0.0im
    0.0778581192025522 + 0.0im
   0.09597378493453936 + 0.0im

julia&gt; history
Converged: 10 of 10 eigenvalues in 174 matrix-vector products

julia&gt; norm(A * decomp.Q - decomp.Q * decomp.R)
6.39386920955869e-8

julia&gt; λs, X = partialeigen(decomp);

julia&gt; norm(A * X - X * Diagonal(λs))
6.393869211477937e-8</code></pre><h2 id="Partial-Schur-decomposition"><a class="docs-heading-anchor" href="#Partial-Schur-decomposition">Partial Schur decomposition</a><a id="Partial-Schur-decomposition-1"></a><a class="docs-heading-anchor-permalink" href="#Partial-Schur-decomposition" title="Permalink"></a></h2><p>The <a href="#ArnoldiMethod.partialschur"><code>partialschur</code></a> method forms the backbone of the package. It computes an approximate, partial Schur decomposition of a matrix <span>$A$</span>:</p><p class="math-container">\[AQ = QR\]</p><p>where <span>$Q$</span> is orthonormal of size <span>$n \times \texttt{nev}$</span> and <span>$R$</span> is upper  triangular of size <span>$\texttt{nev} \times \texttt{nev}.$</span> with eigenvalues of <span>$A$</span> on the diagonal.</p><div class="admonition is-info"><header class="admonition-header">2x2 blocks in real arithmetic</header><div class="admonition-body"><p>In real arithmetic <span>$R$</span> is quasi upper triangular, with <span>$2 \times 2$</span> blocks on the diagonal  corresponding to conjugate complex-valued eigenpairs. These <span>$2 \times 2$</span> blocks are used for efficiency, since otherwise the entire Schur form would have to use complex arithmetic.</p></div></div><div class="admonition is-info"><header class="admonition-header">A partial Schur decomposition is often enough</header><div class="admonition-body"><p>Often a partial Schur decomposition is all you need, cause it&#39;s more stable to compute and work with than a partial eigendecomposition.</p><p>Also note that for complex Hermitian and real symmetric matrices, the partial Schur form coincides with the partial eigendecomposition (the <span>$R$</span> matrix is diagonal).</p></div></div><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ArnoldiMethod.partialschur" href="#ArnoldiMethod.partialschur"><code>ArnoldiMethod.partialschur</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">partialschur(A; nev, which, tol, mindim, maxdim, restarts) → PartialSchur, History</code></pre><p>Find <code>nev</code> approximate eigenpairs of <code>A</code> with eigenvalues near a specified target.</p><p>The matrix <code>A</code> can be any linear map that implements <code>mul!(y, A, x)</code>, <code>eltype</code> and <code>size</code>.</p><p>The method will run iteratively until the eigenpairs are approximated to the prescribed tolerance or until <code>restarts</code> restarts have passed.</p><p><strong>Arguments</strong></p><p>The most important keyword arguments:</p><table><tr><th style="text-align: right">Keyword</th><th style="text-align: left">Type</th><th style="text-align: left">Default</th><th style="text-align: left">Description</th></tr><tr><td style="text-align: right"><code>nev</code></td><td style="text-align: left"><code>Int</code></td><td style="text-align: left"><code>min(6, size(A, 1))</code></td><td style="text-align: left">Number of eigenvalues</td></tr><tr><td style="text-align: right"><code>which</code></td><td style="text-align: left"><code>Symbol</code> or <code>Target</code></td><td style="text-align: left"><code>:LM</code></td><td style="text-align: left">One of <code>:LM</code>, <code>:LR</code>, <code>:SR</code>, <code>:LI</code>, <code>:SI</code>, see below.</td></tr><tr><td style="text-align: right"><code>tol</code></td><td style="text-align: left"><code>Real</code></td><td style="text-align: left"><code>√eps</code></td><td style="text-align: left">Tolerance for convergence: ‖Ax - xλ‖₂ &lt; tol * ‖λ‖</td></tr></table><p>The target <code>which</code> can be any of:</p><table><tr><th style="text-align: right">Target</th><th style="text-align: left">Description</th></tr><tr><td style="text-align: right"><code>:LM</code> or <code>LM()</code></td><td style="text-align: left">Largest magnitude: <code>abs(λ)</code> is largest</td></tr><tr><td style="text-align: right"><code>:LR</code> or <code>LR()</code></td><td style="text-align: left">Largest real part: <code>real(λ)</code> is largest</td></tr><tr><td style="text-align: right"><code>:SR</code> or <code>SR()</code></td><td style="text-align: left">Smallest real part: <code>real(λ)</code> is smallest</td></tr><tr><td style="text-align: right"><code>:LI</code> or <code>LI()</code></td><td style="text-align: left">Largest imaginary part: <code>imag(λ)</code> is largest</td></tr><tr><td style="text-align: right"><code>:SI</code> or <code>SI()</code></td><td style="text-align: left">Smallest imaginary part: <code>imag(λ)</code> is smallest</td></tr></table><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>The targets <code>:LI</code> and <code>:SI</code> only make sense in complex arithmetic. In real arithmetic <code>λ</code> is an eigenvalue iff <code>conj(λ)</code> is an eigenvalue and this  conjugate pair converges simultaneously.</p></div></div><p><strong>Return values</strong></p><p>The function returns a tuple</p><pre><code class="language-julia hljs">decomp, history = partialschur(A, ...)</code></pre><p>where <code>decomp</code> is a <a href="#ArnoldiMethod.PartialSchur"><code>PartialSchur</code></a> struct which  forms a partial Schur decomposition of <code>A</code> to a prescribed tolerance:</p><pre><code class="language-julia hljs">&gt; norm(A * decomp.Q - decomp.Q * decomp.R)</code></pre><p><code>history</code> is a <a href="#ArnoldiMethod.History"><code>History</code></a> struct that holds some basic information about convergence of the method:</p><pre><code class="language-julia hljs">&gt; history.converged
true
&gt; @show history
Converged after 359 matrix-vector products</code></pre><p><strong>Advanced usage</strong></p><p>Further there are advanced keyword arguments for tuning the algorithm:</p><table><tr><th style="text-align: right">Keyword</th><th style="text-align: left">Type</th><th style="text-align: left">Default</th><th style="text-align: left">Description</th></tr><tr><td style="text-align: right"><code>mindim</code></td><td style="text-align: left"><code>Int</code></td><td style="text-align: left"><code>min(max(10, nev), size(A,1))</code></td><td style="text-align: left">Minimum Krylov dimension (≥ nev)</td></tr><tr><td style="text-align: right"><code>maxdim</code></td><td style="text-align: left"><code>Int</code></td><td style="text-align: left"><code>min(max(20, 2nev), size(A,1))</code></td><td style="text-align: left">Maximum Krylov dimension (≥ min)</td></tr><tr><td style="text-align: right"><code>restarts</code></td><td style="text-align: left"><code>Int</code></td><td style="text-align: left"><code>200</code></td><td style="text-align: left">Maximum number of restarts</td></tr></table><p>When the algorithm does not converge, one can increase <code>restarts</code>. When the  algorithm converges too slowly, one can play with <code>mindim</code> and <code>maxdim</code>. It is  suggested to keep <code>mindim</code> equal to or slightly larger than <code>nev</code>, and <code>maxdim</code> is usually about two times <code>mindim</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaLinearAlgebra/ArnoldiMethod.jl/blob/4785a73197a2da006cbdd6643f89c6f99dab7c10/src/run.jl#L14-L93">source</a></section></article><h2 id="Partial-eigendecomposition"><a class="docs-heading-anchor" href="#Partial-eigendecomposition">Partial eigendecomposition</a><a id="Partial-eigendecomposition-1"></a><a class="docs-heading-anchor-permalink" href="#Partial-eigendecomposition" title="Permalink"></a></h2><p>After computing the partial Schur decomposition, it can be transformed into a partial eigendecomposition via the <a href="#ArnoldiMethod.partialeigen"><code>partialeigen</code></a> helper function. The basic math is to determine the eigendecomposition of the upper triangular matrix <span>$RY = Y\Lambda$</span> such that</p><p class="math-container">\[A(QY) = (QY)\Lambda\]</p><p>forms the full eigendecomposition of <span>$A$</span>, where <span>$QY$</span> are the eigenvectors and <span>$\Lambda$</span> is a <span>$\texttt{nev} \times \texttt{nev}$</span> diagonal matrix of eigenvalues.</p><p>This step is a relatively cheap post-processing step.</p><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ArnoldiMethod.partialeigen" href="#ArnoldiMethod.partialeigen"><code>ArnoldiMethod.partialeigen</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">partialeigen(P::PartialSchur) → (Vector{&lt;:Union{Real,Complex}}, Matrix{&lt;:Union{Real,Complex}})</code></pre><p>Transforms a partial Schur decomposition into an eigendecomposition.</p><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>For real-symmetric and complex-Hermitian matrices the Schur vectors coincide with the eigenvectors and the R matrix is diagonal, and hence it is not necessary to call this function in that case.</p><p>In fact, in case of real-symmetric and complex-Hermitian matrices <em>with repeated eigenvalues</em>, calling <code>partialeigen</code> may be undesirable, as it can return eigenvectors that are not orthogonal. The Schur vectors on the other hand are orthogonal by construction.</p></div></div><p>The method still relies on LAPACK to compute the eigenvectors of the (quasi) upper triangular matrix <code>R</code> from the partial Schur decomposition.</p><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>This method is currently type unstable for real matrices, since we have not yet decided how to deal with complex conjugate pairs of eigenvalues. E.g. if almost all eigenvalues are real, but there are just a few conjugate  pairs, should all eigenvectors be complex-valued?</p></div></div></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaLinearAlgebra/ArnoldiMethod.jl/blob/4785a73197a2da006cbdd6643f89c6f99dab7c10/src/eigvals.jl#L67-L91">source</a></section></article><h2 id="Stopping-criterion"><a class="docs-heading-anchor" href="#Stopping-criterion">Stopping criterion</a><a id="Stopping-criterion-1"></a><a class="docs-heading-anchor-permalink" href="#Stopping-criterion" title="Permalink"></a></h2><p>ArnoldiMethod.jl considers an approximate eigenpair converged when the  condition</p><p class="math-container">\[\|Ax - x\lambda\|_2 &lt; \texttt{tol}|\lambda|\]</p><p>is met, where <code>tol</code> is a user provided tolerance. Note that this stopping  criterion is scale-invariant. For a scaled matrix <span>$B = \alpha A$</span> the same  approximate eigenvector together with the scaled eigenvalue <span>$\alpha\lambda$</span>  would satisfy the stopping criterion.</p><h2 id="The-PartialSchur-and-History-structs"><a class="docs-heading-anchor" href="#The-PartialSchur-and-History-structs">The PartialSchur and History structs</a><a id="The-PartialSchur-and-History-structs-1"></a><a class="docs-heading-anchor-permalink" href="#The-PartialSchur-and-History-structs" title="Permalink"></a></h2><p>For completeness, the return values of the <a href="#ArnoldiMethod.partialschur"><code>partialschur</code></a> function:</p><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ArnoldiMethod.PartialSchur" href="#ArnoldiMethod.PartialSchur"><code>ArnoldiMethod.PartialSchur</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">PartialSchur(Q, R, eigenvalues)</code></pre><p>Holds an orthonormal basis <code>Q</code> and a (quasi) upper triangular matrix <code>R</code>.</p><p>For convenience the eigenvalues that appear on the diagonal of <code>R</code> are also  listed as <code>eigenvalues</code>, which is in particular useful in the case of real  matrices with complex eigenvalues. Note that the eigenvalues are always  complex, even when the matrix <code>R</code> is real.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaLinearAlgebra/ArnoldiMethod.jl/blob/4785a73197a2da006cbdd6643f89c6f99dab7c10/src/ArnoldiMethod.jl#L57-L66">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ArnoldiMethod.History" href="#ArnoldiMethod.History"><code>ArnoldiMethod.History</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">History(mvproducts, nconverged, converged, nev)</code></pre><p>History shows whether the method has converged (when <code>nconverged</code> ≥ <code>nev</code>) and how many matrix-vector products were necessary to do so.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaLinearAlgebra/ArnoldiMethod.jl/blob/4785a73197a2da006cbdd6643f89c6f99dab7c10/src/run.jl#L144-L149">source</a></section></article><h2 id="What-algorithm-is-ArnoldiMethod.jl?"><a class="docs-heading-anchor" href="#What-algorithm-is-ArnoldiMethod.jl?">What algorithm is ArnoldiMethod.jl?</a><a id="What-algorithm-is-ArnoldiMethod.jl?-1"></a><a class="docs-heading-anchor-permalink" href="#What-algorithm-is-ArnoldiMethod.jl?" title="Permalink"></a></h2><p>The underlying algorithm is the restarted Arnoldi method, which be viewed as a mix between a subspace accelerated version of the power method and a truncated  version of the dense QR algorithm.</p><p>Initially the method was based on the <em>Implicitly Restarted Arnoldi Method</em> (or IRAM for short), which is the algorithm implemented by ARPACK. This method has a very elegant restarting scheme based on exact QR iterations, but is  unfortunately susceptible to forward instabilities of the QR algorithm.</p><p>For this reason the <em>Krylov-Schur</em> method is currently embraced in this package, which is mathematically equivalent to IRAM, but has better stability by  replacing exact QR iterations with a direct method that reorders the Schur form. In fact we see Krylov-Schur just as an implementation detail of the Arnoldi  method.</p><h2 id="Bringing-problems-to-standard-form"><a class="docs-heading-anchor" href="#Bringing-problems-to-standard-form">Bringing problems to standard form</a><a id="Bringing-problems-to-standard-form-1"></a><a class="docs-heading-anchor-permalink" href="#Bringing-problems-to-standard-form" title="Permalink"></a></h2><p>ArnoldiMethod.jl by default can only compute an approximate, partial Schur decomposition <span>$AQ = QR$</span> and (from there) a partial eigendecomposition <span>$AX = XD$</span> of a matrix <span>$A$</span>, for <em>extremal</em> eigenvalues <span>$d_{ii}$</span>. These are eigenvalues at the boundary of the convex hull of the spectrum of <span>$A$</span>. Search targets for eigenvalues are: large magnitude, and large or small real or imaginary parts.</p><p>Whenever one targets eigenvalues close to a specific point in the complex plane, or whenever one solves generalized eigenvalue problems, suitable transformations will enable you to recast the problem into something that ArnoldiMethod.jl can  handle well. In this section we provide some examples.</p><h3 id="Shift-and-invert-with-LinearMaps.jl"><a class="docs-heading-anchor" href="#Shift-and-invert-with-LinearMaps.jl">Shift-and-invert with LinearMaps.jl</a><a id="Shift-and-invert-with-LinearMaps.jl-1"></a><a class="docs-heading-anchor-permalink" href="#Shift-and-invert-with-LinearMaps.jl" title="Permalink"></a></h3><p>To find eigenvalues closest to the origin of <span>$A$</span>, one can find the eigenvalues of largest magnitude of <span>$A^{-1}$</span>. <a href="https://github.com/Jutho/LinearMaps.jl">LinearMaps.jl</a>  is a neat way to implement this.</p><pre><code class="language-julia hljs">using ArnoldiMethod, LinearAlgebra, LinearMaps

# Define a matrix whose eigenvalues you want
A = rand(100,100)

# Factorizes A and builds a linear map that applies inv(A) to a vector.
function construct_linear_map(A)
    F = factorize(A)
    LinearMap{eltype(A)}((y, x) -&gt; ldiv!(y, F, x), size(A,1), ismutating=true)
end

# Target the largest eigenvalues of the inverted problem
decomp, = partialschur(construct_linear_map(A), nev=4, tol=1e-5, restarts=100, which=:LM)
λs_inv, X = partialeigen(decomp)

# Eigenvalues have to be inverted to find the smallest eigenvalues of the non-inverted problem.
λs = 1 ./ λs_inv
 
# Show that Ax = xλ
@show norm(A * X - X * Diagonal(λs)) # 7.38473677258669e-6</code></pre><h3 id="Shift-and-invert-for-generalized-eigenvalue-problems"><a class="docs-heading-anchor" href="#Shift-and-invert-for-generalized-eigenvalue-problems">Shift-and-invert for generalized eigenvalue problems</a><a id="Shift-and-invert-for-generalized-eigenvalue-problems-1"></a><a class="docs-heading-anchor-permalink" href="#Shift-and-invert-for-generalized-eigenvalue-problems" title="Permalink"></a></h3><p>When targeting the eigenvalues closest to the origin of a generalized eigenvalue problem <span>$Ax = Bx\lambda$</span>, one can apply the shift-and-invert trick, recasting  the problem to <span>$A^{-1}Bx = x\theta$</span> where <span>$\lambda = 1 / \theta$</span>.</p><pre><code class="language-julia hljs">using ArnoldiMethod, LinearAlgebra, LinearMaps

# Define the matrices of the generalized eigenvalue problem
A, B = rand(100, 100), rand(100, 100)

struct ShiftAndInvert{TA,TB,TT}
    A_lu::TA
    B::TB
    temp::TT
end

function (M::ShiftAndInvert)(y, x)
    mul!(M.temp, M.B, x)
    ldiv!(y, M.A_lu, M.temp)
end

function construct_linear_map(A, B)
    a = ShiftAndInvert(factorize(A), B, Vector{eltype(A)}(undef, size(A, 1)))
    LinearMap{eltype(A)}(a, size(A, 1), ismutating = true)
end

# Target the largest eigenvalues of the inverted problem
decomp, = partialschur(
    construct_linear_map(A, B),
    nev = 4,
    tol = 1e-5,
    restarts = 100,
    which = :LM,
)
λs_inv, X = partialeigen(decomp)

# Eigenvalues have to be inverted to find the smallest eigenvalues of the non-inverted problem.
λs = 1 ./ λs_inv

# Show that Ax = Bxλ
@show norm(A * X - B * X * Diagonal(λs)) # 2.8043149027575927e-6</code></pre><h3 id="Largest-eigenvalues-of-a-generalized-eigenvalue-problem-with-symmetric-positive-definite-B"><a class="docs-heading-anchor" href="#Largest-eigenvalues-of-a-generalized-eigenvalue-problem-with-symmetric-positive-definite-B">Largest eigenvalues of a generalized eigenvalue problem with symmetric positive-definite B</a><a id="Largest-eigenvalues-of-a-generalized-eigenvalue-problem-with-symmetric-positive-definite-B-1"></a><a class="docs-heading-anchor-permalink" href="#Largest-eigenvalues-of-a-generalized-eigenvalue-problem-with-symmetric-positive-definite-B" title="Permalink"></a></h3><p>When <span>$B$</span> is a symmetric positive-definite matrix, and it&#39;s affordable to compute a Cholesky decomposition of <span>$B$</span>, one can use ArnoldiMethod.jl to create a partial Schur decomposition of <span>$A$</span> where the Schur vectors are <span>$B$</span>-orthonormal:</p><p>Solve <span>$Q^*AQ = R$</span> where <span>$Q^*BQ = I$</span> and <span>$R$</span> is upper triangular. If <span>$A = A^*$</span> as well, then <span>$R$</span> is diagonal and we have a partial eigendecomposition of <span>$A$</span>.</p><p>First we take the Cholesky decomposition <span>$B = LL^*$</span> and plug it into <span>$AQ = BQR$</span> to obtain <span>$L^{-*}AL^{-1}L^*Q = L^*QR$</span>.</p><p>Then define <span>$C = L^{-*}AL^{-1}$</span> and <span>$Y = L^*Q$</span> and we have a standard Schur decomposition <span>$CY = YR$</span> which we can solve using <code>partialschur</code>.</p><p>The linear map <span>$C$</span> can be defined as follows:</p><pre><code class="language-julia hljs">using ArnoldiMethod, LinearAlgebra, LinearMaps
struct WithBInnerProduct{TA,TL}
    A::TA
    L::TL
end

function (M::WithBInnerProduct)(y, x)
    # Julia unfortunately does not have in-place CHOLMOD solve, so this is far from optimal.
    tmp = M.L \ x
    mul!(y, M.A, tmp)
    y .= M.L&#39; \ y
    return y
end

# Define the matrices of the generalized eigenvalue problem
A = rand(100, 100)
B = Diagonal(range(1.0, 2.0, length = 100))

# Reformulate the problem as standard Schur decomposition
F = cholesky(B)
linear_map = LinearMap{eltype(A)}(WithBInnerProduct(A, F.L), size(A, 1), ismutating = true)
decomp, info = partialschur(linear_map, nev = 4, which = :LM, tol = 1e-10)

# Translate back to the original problem
Q = F.L&#39; \ decomp.Q

@show norm(Q&#39; * A * Q - decomp.R)  # 3.883933945390996e-14
@show norm(Q&#39; * B * Q - I)  # 3.1672155003480583e-15</code></pre><h2 id="Goal-of-this-package:-an-efficient,-pure-Julia-implementation"><a class="docs-heading-anchor" href="#Goal-of-this-package:-an-efficient,-pure-Julia-implementation">Goal of this package: an efficient, pure Julia implementation</a><a id="Goal-of-this-package:-an-efficient,-pure-Julia-implementation-1"></a><a class="docs-heading-anchor-permalink" href="#Goal-of-this-package:-an-efficient,-pure-Julia-implementation" title="Permalink"></a></h2><p>This project started with two goals:</p><ol><li>Having a <em>native</em> Julia implementation of the <code>eigs</code> function that performs as well as ARPACK. With native we mean that its implementation should be generic and support any number type. Currently the <a href="#ArnoldiMethod.partialschur"><code>partialschur</code></a> function does not depend on LAPACK, it even has its own implementation of a dense eigensolver.</li><li>Removing the dependency of the Julia language on ARPACK. This goal was already achieved before the package was stable enough, since ARPACK moved to a separate repository <a href="https://github.com/JuliaLinearAlgebra/Arpack.jl/">Arpack.jl</a>.</li></ol><h2 id="Performance"><a class="docs-heading-anchor" href="#Performance">Performance</a><a id="Performance-1"></a><a class="docs-heading-anchor-permalink" href="#Performance" title="Permalink"></a></h2><p>ArnoldiMethod.jl should be roughly on par with Arpack.jl, and slightly faster than KrylovKit.jl.</p><p>Do note that for an apples to apples comparison, it&#39;s important to compare with identical defaults: each of the mentioned packages uses a slightly different default convergence criterion.</p><h2 id="Status"><a class="docs-heading-anchor" href="#Status">Status</a><a id="Status-1"></a><a class="docs-heading-anchor-permalink" href="#Status" title="Permalink"></a></h2><p>An overview of what we have, how it&#39;s done and what we&#39;re missing.</p><h3 id="Implementation-details"><a class="docs-heading-anchor" href="#Implementation-details">Implementation details</a><a id="Implementation-details-1"></a><a class="docs-heading-anchor-permalink" href="#Implementation-details" title="Permalink"></a></h3><ul><li>The method does not make assumptions about the type of the matrix; it is  matrix-free.</li><li>Converged Ritz vectors are locked (or deflated).</li><li>We may do &quot;purging&quot; differently from ARPACK: in ArnoldiMethod.jl it is rather &quot;unlocking&quot;, in the sense that converged but unwanted eigenvectors are retained in the search subspace instead of removed from it.</li><li>Important matrices and vectors are pre-allocated and operations on the  Hessenberg matrix are in-place; Julia&#39;s garbage collector can sit back.</li><li>Krylov basis vectors are orthogonalized with repeated classical Gram-Schmidt to ensure they are orthogonal up to machine precision; this is a BLAS-2 operation.</li><li>To compute the Schur decomposition of the Hessenberg matrix we use a dense  QR algorithm written natively in Julia. It is based on implicit (or Francis)  shifts and handles real arithmetic efficiently.</li><li>Locking and purging of Ritz vectors is done by reordering the Schur form,  which is also implemented natively in Julia. In the real case it is done by casting tiny Sylvester equations to linear systems and solving them with  complete pivoting.</li><li>Shrinking the size of the Krylov subspace and changing its basis is done by accumulating all rotations and reflections in a unitary matrix <code>Q</code>, and then simply computing the matrix-matrix product <code>V := V * Q</code>, where <code>V</code> is the  original orthonormal basis. This is not in-place in <code>V</code>, so we allocate a bit of scratch space ahead of time.</li></ul><h3 id="Not-implemented"><a class="docs-heading-anchor" href="#Not-implemented">Not implemented</a><a id="Not-implemented-1"></a><a class="docs-heading-anchor-permalink" href="#Not-implemented" title="Permalink"></a></h3><ul><li>Being able to kickstart the method from a given Arnoldi relation. This also captures:<ol><li>Making an initial guess by providing a known approximate eigenvector;</li><li>Deflating some subspace by starting the Arnoldi method with a given partial Schur decomposition.</li></ol></li><li>Matrix-induced inner product for generalized eigenvalue problems.</li><li>Efficient implementation of symmetric problems with Lanczos.</li></ul><p>On my wish list is to allow custom vector or matrix types, so that we can  delegate expensive but trivial work to hardware that can do it faster  (distributed memory / GPU). The basic concept would be: </p><ol><li>The core Arnoldi method performs tedious linear algebra on the projected,  low-dimensional problem, but finally just outputs a change of basis in the form of a unitary matrix Q.</li><li>Appropriate hardware does the change of basis <code>V := V * Q</code>.</li></ol><p>Similar things should happen for expansion of the subspace and  orthogonalization.</p></article><nav class="docs-footer"><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="auto">Automatic (OS)</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.2.1 on <span class="colophon-date" title="Saturday 17 February 2024 23:27">Saturday 17 February 2024</span>. Using Julia version 1.10.1.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
